# 主库挂了，如何不不间断服务

<center><img src="https://ss.im5i.com/2021/07/31/GoLCS.png" alt="GoLCS.png" border="0" /></center>

无论是写服务中断，还是从库无法进行数据同步，都是不能接受的。所以，如果主库挂了，就需要运行一个新主库，比如说把一个从库切换为主库，把它当成主库。这就涉及到三个问题：

- 主库真的挂了吗?
- 该选择哪个从库作为主库?
- 怎么把新主库的相关信息通知给从库和客户端呢?

这就要提到哨兵机制了。在Redis主从集群中，**哨兵机制是实现主从库自动切换的关键机制**，它有效地解决了主从复制模式下**故障转移**的这三个问题。

# 哨兵

## 简介

哨兵(sentinel)是一个分布式系统，用于对主从结构中的每台服务器进行<span style="color:red">监控</span>，当出现故障时通过<span style="color:red">投票机制选择</span>新的master并将所有slave连接到新的master。

## 作用

**监控：**

- 不断的检查master和slave是否正常运行。master存活检测、master与slave运行情况检测

**通知（提醒)：**

- 当被监控的服务器出现问题时，向其他(哨兵间，客户端）发送通知。

**自动故障转移：**

- 断开master与slave连接，选取一个slave作为master，将其他slave连接到新的master，并告知客户端新的服务器地址

注意：

- 哨兵也是一台redis服务器，只是不提供数据服务
- 通常哨兵配置数量为单数

# Redis哨兵机制的基本流程

哨兵其实就是一个运行在特殊模式下的Redis进程，主从库实例运行的同时，它也在运行。<span style="color:green">哨兵主要负责的就是三个任务：监控、选主(选择主库)和通知</span>。

**监控**是指哨兵进程在运行时，**周期性地给所有的主从库发送PING命令，检测它们是否仍然在线运行**。如果从库没有在规定时间内响应哨兵的PING命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的PING命令，哨兵就会判定主库下线，然后开始**自动切换主库**的流程。

这个流程首先是执行哨兵的第二个任务，**选主**。主库挂了以后，哨兵就需要从很多个从库里，**按照一定的规则选择一个从库实例**，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。

然后，哨兵会执行最后一个任务：**通知**。在执行通知任务时，**哨兵会把新主库的连接信息发给其他从库，让它们执行`replicaof`命令，和新主库建立连接，并进行数据复制**。同时，**哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上**。

<center><img src="https://ss.im5i.com/2021/07/31/GoecL.png" alt="GoecL.png" border="0" /></center>

在这三个任务中，通知任务相对来说比较简单，哨兵只需要把新主库信息发给从库和客户端，让它们和新主库建立连接就行，**并不涉及决策的逻辑**。但是，在监控和选主这两个任务中，哨兵需要做出两个决策:

- <span style="color:blue">在监控任务中，哨兵需要判断主库是否处于下线状态</span>

- <span style="color:blue">在选主任务中，哨兵也要决定选择哪个从库实例作为主库</span>

# 主观下线和客观下线

**"主观下线"：**

哨兵进程会使用PING命令检测它自己和主、从库的网络连接情况，用来判断实例的状态。

- 如果检测的是从库，那么，哨兵简单地把它标记为“主观下线”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断
- 如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判了，其实主库并没有故障。**因为，一旦启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销**。误判就是主库实际并没有下线，但是哨兵误以为它下线了。**误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下。**

哨兵机制通常会采用多实例组成的集群模式进行部署，这也被称为**哨兵集群**。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。

**在判断主库是否下线时，不能由一个哨兵说了算，只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”**，这个叫法也是表明主库下线成为一个客观事实了。这个判断原则就是：<span style="color:red">少数服从多数</span>。同时，这会进一步触发哨兵开始主从切换流程。

如下图所示，Redis 主从集群有一个主库、三个从库，还有三个哨兵实例。在图片的左边，哨兵2判断主库为“主观下线”，但哨兵1和3却判定主库是上线状态，此时，主库仍然被判断为处于上线状态。在图片的右边，哨兵1和⒉都判断主库为“主观下线”，此时，即使哨兵3仍然判断主库为上线状态，主库也被标记为“客观下线”了。

<center><img src="https://ss.im5i.com/2021/07/31/Goz3t.png" alt="Goz3t.png" border="0" /></center>

简单来说，“客观下线”的标准就是，当有$N$个哨兵实例时，最好要有N/2＋1个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以**减少误判的概率**，也能避免误判带来的无谓的主从库切换。(当然，有多少个实例做出“主观下线”的判断才可以，可以由Redis管理员自行设定)。

# 如何选定新主库？

一般来说，哨兵选择新主库的过程称为**“筛选＋打分”**。简单来说，在多个从库中，先按照一定的**筛选条件**，把不符合条件的从库去掉。然后，我们再按照**一定的规则**，给剩下的从库逐个打分，**将得分最高的从库选为新主库**，如下图所示：

<center><img src="https://ss.im5i.com/2021/07/31/Go0kq.png" alt="Go0kq.png" border="0" /></center>

**筛选条件：**
一般情况下，肯定要先保证所选的从库仍然在线运行。不过，在选主时从库正常在线，这只能表示从库的现状良好，并不代表它就是最适合做主库的。
**在选主时，除了要检查从库的当前在线状态，还要判断它之前的网络连接状态**。如果从库总是和主库断连，而且断连次数超出了一定的阈值，我们就有理由相信，这个从库的网络状况并不是太好，就可以把这个从库筛掉了。`down-after-milliseconds` 是我们认定**主从库断连的最大连接超时时间**。如果在`down-after-milliseconds`毫秒内，主从节点都没有通过网络联系上，就可以认为主从节点断连了。**如果发生断连的次数超过了10次，就说明这个从库的网络状况不好，不适合作为新主库。**这样我们就过滤掉了不适合做主库的从库，完成了筛选工作。

**打分：**

接下来就要给剩余的从库打分了。可以分别按照三个规则依次进行三轮打分，这三个规则分别是<span style="color:red">从库优先级、从库复制进度以及从库ID号</span>。只要**在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束**。如果没有出现得分最高的从库，那么就继续进行下一轮。

- **第一轮：优先级最高的从库得分高**

用户可以通过`slave-priority`配置项，给不同的从库设置不同优先级。比如，有两个从库，它们的内存大小不一样，可以手动给内存大的实例设置一个高优先级。在选主时，哨兵会给优先级高的从库打高分，如果有一个从库优先级最高，那么它就是新主库了。如果从库的优先级都一样，那么哨兵开始第二轮打分。

- **第二轮：和旧主库同步程度最接近的从库得分高**

这个规则的依据是，如果选择和旧主库同步最接近的那个从库作为主库，那么，这个新主库上就有最新的数据。

此时，想要找的从库，它的`slave_repl_offset`需要最接近`master_repl_offset`。如果在所有从库中，有从库的`slave_repl_offset`最接近`master_repl_offset`，那么它的得分就最高，可以作为新主库。

就像下图所示，旧主库的`master_repl_offset`是 1000，从库1、2和3的`slave_repl_offset`分别是950、990和900，那么，从库⒉就应该被选为新主库。

<center><img src="https://ss.im5i.com/2021/07/31/Goafm.png" alt="Goafm.png" border="0" /></center>

当然，如果有两个从库的`slave_repl_offset`值大小是一样的（例如，从库1和从库2的`slave_repl_offset`值都是990)，就需要给它们进行第三轮打分了。

- **第三轮：ID号小的从库得分高**

每个实例都会有一个ID，这个ID就类似于这里的从库的编号。目前，Redis在选主库时，有一个默认的规定：在优先级和复制进度都相同的情况下，**ID号最小的从库得分最高**，会被选为新主库。

到这里，新主库就被选出来了，“选主”这个过程就完成了。

再回顾下这个流程。首先，哨兵会按照在线状态、网络状态，筛选过滤掉一部分不符合要求的从库，然后，依次按照优先级、复制进度、ID号大小再对剩余的从库进行打分，只要有得分最高的从库出现，就把它选为新主库

# 哨兵集群存在的问题

使用多个哨兵实例组成一个哨兵集群可以降低误判率，但也会因此面临着—些新的挑战，例如：

- 哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗?

- 哨兵集群多数实例达成共识，判断出主库“客观下线”后，由哪个实例来执行主从切换呢?

**实际上，一旦多个实例组成了哨兵集群，即使有哨兵实例出现故障挂掉了，其他哨兵还能继续协作完成主从库切换的工作，包括判定主库是不是处于下线状态，选择新主库，以及通知从库和客户端**。

部署哨兵集群时，在配置哨兵的信息时，只需要用到下面的这个配置项，设置主库的IP和端口，并没有配置其他哨兵的连接信息：

```squirrel
sentinel monitor <master-name> <ip> <redis-port> <quorum>
```

# 基于pub/sub机制的哨兵集群组成

哨兵实例之间可以**相互发现**，要归功于Redis提供的 pub/sub机制，也就是**发布/订阅机制**。

哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息(IP和端口)。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。**当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的IP地址和端口**。

除了哨兵实例，我们自己编写的应用程序也可以通过Redis进行消息的发布和订阅。所以，为了区分不同应用的消息，**Redis 会以频道的形式，对这些消息进行分门别类的管理**。所谓的频道，实际上就是消息的类别。当消息类别相同时，它们就属于同一个频道。反之，就属于不同的频道。**只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换**。

在主从集群中，主库上有一个名为**"\__sentinel__:hello"**的频道，不同哨兵就是通过它来相互发现，实现互相通信的。

在下图中，哨兵1把自己的IP (172.16.19.3）和端口(26579）发布到“\__sentinel__:hello”频道上，哨兵2和3订阅了该频道。那么此时，哨兵2和3就可以从这个频道直接获取哨兵1的IP地址和端口号。

然后，哨兵2、3可以和哨兵1建立网络连接。通过这个方式，哨兵2和3也可以建立网络连接，这样一来，哨兵集群就形成了。它们相互间可以通过网络连接进行通信，比如说对主库有没有下线这件事儿进行判断和协商。

<center><img src="https://ss.im5i.com/2021/07/31/G3rln.png" alt="G3rln.png" border="0" /></center>

**哨兵除了彼此之间建立起连接形成集群外，还需要和从库建立连接**。这是因为，在哨兵的监控任务中，它需要对主从库都进行心跳判断，而且在主从库切换完成后，它还需要通知从库，让它们和新主库进行同步。

那么，**哨兵是如何知道从库的IP地址和端口的呢?**

这是由哨兵向主库发送`INFO`命令来完成的。就像下图所示，哨兵2给主库发送`INFO`命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵1和3可以通过相同的方法和从库建立连接。

<center><img src="https://ss.im5i.com/2021/07/31/G3y9l.png" alt="G3y9l.png" border="0" /></center>

通过 pub/sub机制，哨兵之间可以组成集群，同时，哨兵又通过INFO命令，获得了从库连接信息，也能和从库建立连接，并进行监控了。

但是，**哨兵不能只和主、从库连接**。因为，主从库切换后，客户端也需要知道新主库的连接信息，才能向新主库发送请求操作。所以，**哨兵还需要完成把新主库的信息告诉客户端这个任务**。

而且，在实际使用哨兵时，有时会遇到这样的问题：如何在客户端通过监控了解哨兵进行主从切换的过程呢? 比如说，主从切换进行到哪一步了? 这其实就是要求，客户端能够获取到哨兵集群在监控、选主、切换这个过程中发生的各种事件。

此时，仍然可以依赖pub/sub机制，来帮助**完成哨兵和客户端间的信息同步**。

# 基于pub/sub机制的客户端事件通知

从本质上说，哨兵就是一个运行在特定模式下的Redis实例，只不过它并不服务请求操作，只是完成**监控、选主和通知**的任务。所以，**每个哨兵实例也提供 pub/sub机制，客户端可以从哨兵订阅消息**。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。

<center><img src="https://ss.im5i.com/2021/07/31/G3Ba7.png" alt="G3Ba7.png" border="0" /></center>

知道了这些频道之后，就可以让客户端从哨兵这里订阅消息了。具体的操作步骤是，客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，可以在客户端执行订阅命令，来获取不同的事件消息。

例如：可以执行如下命令，来订阅“所有实例进入客观下线状态的事件”︰

```squirrel
SUBSCRIBE +odown
```

如下命令，可以订阅所有事件：

```squirrel
PSUBSCRIBE *
```

当哨兵把新主库选择出来后，客户端就会看到下面的`switch-master`事件。这个事件表示主库已经切换了，新主库的IP地址和端口信息已经有了。这个时候，客户端就可以用这里面的新主库地址和端口进行通信了。

```squirrel
switch-master <master name> <oldip> <oldport> <newip> <newport>
```

有了 pub/sub机制，哨兵和哨兵之间、哨兵和从库之间、哨兵和客户端之间就都能建立起连接了，再加上主库下线判断和选主依据，哨兵集群的监控、选主和通知三个任务就基本可以正常工作了。不过，还需要考虑一个问题：主库故障以后，哨兵集群有多个实例，那怎么确定由哪个哨兵来进行实际的主从切换呢?

# 由哪个哨兵执行主从切换？

确定由哪个哨兵执行主从切换的过程，和主库“客观下线”的判断过程类似，也是一个“**投票仲裁**”的过程。在具体了解这个过程前，再来看下，判断“客观下线”的仲裁过程。

哨兵集群要判定主库“客观下线”，需要有一定数量的实例都认为该主库已经“主观下线”了。接下来，介绍下具体的判断过程。

任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 `is-master-down-by-addr`命令。接着，其他实例会根据自己和主库的连接情况，做出Y或N的响应，Y相当于赞成票，N相当于反对票。

<center><img src="https://ss.im5i.com/2021/07/31/G3cG2.png" alt="G3cG2.png" border="0" /></center>

一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的`quorum`配置项设定的。例如，现在有5个哨兵，quorum 配置的是3，那么，一个哨兵需要3张赞成票（N/2 + 1），就可以标记主库为“客观下线”了。这3张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。

此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“**Leader选举**”。因为最终执行主从切换的哨兵称为Leader，投票过程就是确定Leader。

<span style="color:red">在投票过程中，任何一个想成为Leader的哨兵，要满足两个条件：第一，拿到半数以上的赞成票（即要求数量大于N/2+1）；第二，拿到的票数同时还需要大于等于哨兵配置文件中的`quorum`值</span>。以3个哨兵为例，假设此时的`quorum`设置为2，那么，任何一个想成为Leader的哨兵只要拿到2张赞成票，就可以了。

哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。

需要注意的是，如果哨兵集群只有2个实例，此时，一个哨兵要想成为Leader，必须获得2票，而不是1票。所以，如果有个哨兵挂掉了，那么，此时的集群是无法进行主从库切换的。因此，**通常至少会配置3个哨兵实例**。并且，<span style="color:red">要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值`down-after-milliseconds`</span>

**注：**

- 不同哨兵的网络连接、系统压力不完全一样，接收到下线协商消息的时间也可能不同，所以，它们同时做出主库客观下线判定的概率较小，一般都有个先后关系

- 哨兵对主从库进行的在线状态检查等操作，是属于一种时间事件，用一个定时器来完成，一般来说每100ms执行一次这些事件。**每个哨兵的定时器执行周期都会加上一个小小的随机时间偏移，目的是让每个哨兵执行上述操作的时间能稍微错开些，也是为了避免它们都同时判定主库下线，同时选举Leader**。

- 即使出现了都投给自己一票的情况，导致无法选出Leader，哨兵会停一段时间(一般是故障转移超时时间failover_timeout的2倍)，然后再可以进行下一轮投票。

- **哨兵如果没有给自己投票，就会把票投给第一个给它发送投票请求的哨兵。后续再有投票请求来，哨兵就拒接投票了。**



